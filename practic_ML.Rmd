---
title: "Practical Machine Learning - Human Activity Recognition"
author: "Paulo Faleiros"
date: "January 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The objective of this project is to generate a model to predict the manner a certain individual performs a weight lift exercise. The exercise can be done correctly (class A) or with 4 different types of flaws (classes B, C, D and E). Based on data collected by sensors in athletes' arms, foreams, belts and in the dumbell, the model, after been trained, needs to be able to predict the exercise class for 20 test cases.

```{r echo=FALSE, results="hide", message=FALSE}
# load libraries
library(knitr)
library(kableExtra)
library(ggplot2)
library(ISLR)
library(caret)
library(kernlab)
library(e1071)
library(Hmisc)
library(gbm)
library(gridExtra)
library(dplyr)

# set seed to ensure reproducibility
set.seed(20190114)

```

## Data Preparation

Read data

```{r echo=TRUE}
setwd("C:/Users/paulo.faleiros/Google Drive/Coursera/courses/JHU-DS/code")
trainingRdat = read.csv("../data/pml-training.csv", stringsAsFactors = FALSE)
validation = read.csv("../data/pml-testing.csv", stringsAsFactors = FALSE)

```

separate train, test and validation datasets
```{r echo=TRUE, cache=TRUE}
pmlData = createDataPartition(y=trainingRdat$classe, p=0.7, list=FALSE)
training = trainingRdat[pmlData,]
testing = trainingRdat[-pmlData,]

```

Produce tidy datasets - eliminate noise by removing exercise metadata and variables whose number of valid observations is less than 95% of the dataset. Apply same cleansing criteria to testing and validation datasets.

```{r echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE}
training[training=="" || training==" " || training=="NA" || training=="#DIV/0!"]<-NA
testing[testing=="" || testing==" " || testing=="NA" || testing=="#DIV/0!"]<-NA

a<-sapply(training[,c(8:159)], function(x) sum(is.na(as.numeric(x))))
predVars<-names(a[((dim(training)[1]-a)/dim(training)[1] > .95)])
newTraining <- training[,predVars]
newTraining$classe <- as.factor(training$classe)

newTesting <- testing[,predVars]
newTesting$classe <- as.factor(testing$classe)

newValidation <- validation[,predVars]

```

## Model creation

In this section, some classification models will be evaluated and the best will be selected to conduct the exercise class prediction.

#### CART model
First try will be done with a simple Classification And Regression Trees (CART) model. Train a CART model with all predictors and generate a prediction with the testing set.
```{r echo=TRUE}
system.time({ fit.rpart <- train(classe ~ ., data=newTraining, method="rpart") })
c<-confusionMatrix(newTesting$classe, predict(fit.rpart, newTesting))
kable(c$table) %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Model considerations:

* Model wasn't able to predict class "D".
* CART model accuracy: `r round(fit.rpart$results[which(fit.rpart$results$cp==fit.rpart$bestTune$cp),2], 3)` is low and Kappa: `r round(fit.rpart$results[which(fit.rpart$results$cp==fit.rpart$bestTune$cp),3], 3)` indicates a fair level of agreement.
* The predictor applied in the testing dataset produced a RMSE of `r RMSE(as.numeric(predict(fit.rpart, newTesting)), 
	 as.numeric(newTesting$classe), na.rm=TRUE)`.

#### Stochastic Gradient Boost model
Previous try demonstrated the data is too complex to generate a simple decision tree model, hence it demands more complex techniques to choose the model. Next try will use a boosting model with cross validation, to improve accuracy. 

Train a Stochastic Gradient boosting model and generate a prediction with the testing set. Cross validation is applied with 5 k-folds.

```{r echo=TRUE, cache=TRUE}
tc <- trainControl(method = 'cv', number = 5, summaryFunction=defaultSummary)
system.time({ fit.gbm <- train(classe ~ ., data=newTraining, method="gbm", 
							   trControl=tc, verbose=FALSE) })
plot(fit.gbm)
```

The chart demonstrates accuracy hits its peak with `r fit.gbm$bestTune$interaction.depth` Max Tree Depth and `r fit.gbm$bestTune$n.trees` boosting interactions:

```{r echo=TRUE}
kable(fit.gbm$results) %>% kable_styling("striped", full_width = F) %>% row_spec(which(fit.gbm$results$interaction.depth ==
								fit.gbm$bestTune$interaction.depth
							& fit.gbm$results$n.trees == 
								fit.gbm$bestTune$n.trees), bold = T, color = "silver", background = "navy")
```


```{r echo=TRUE}
c<-confusionMatrix(newTesting$classe, predict(fit.gbm, newTesting))
kable(c$table) %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Model considerations:

* GBM model accuracy: `r round(fit.gbm$results[which(fit.gbm$results$interaction.depth ==
								fit.gbm$bestTune$interaction.depth
							& fit.gbm$results$n.trees == 
								fit.gbm$bestTune$n.trees),5], 3)` is high and Kappa: `r round(fit.gbm$results[which(fit.gbm$results$interaction.depth ==
								fit.gbm$bestTune$interaction.depth
							& fit.gbm$results$n.trees == 
								fit.gbm$bestTune$n.trees),6], 3)` indicates a substantial level of agreement.
* The predictor applied in the testing dataset produced a RMSE of `r RMSE(as.numeric(predict(fit.gbm, newTesting)), 
	 as.numeric(newTesting$classe), na.rm=TRUE)`, which is less than the CART model.

## Validation dataset

The GBM boosting model looks much better than the CART when comparing accuracy, Kappa and root mean squared error, hence the predictions will be gotten from this model.

```{r  echo=TRUE}

predictions<-predict(fit.gbm, validation)
print(as.character(predictions))
```

## Conclusion

The Stochastic Gradient Boost model choice produced a fairly more accurate model, but the processing time to train it may raise a scalability issue. The model can be tuned by simplifying the predictors, and the Variables Importance analysis demonstrates it:

```{r  echo=FALSE, fig.height=8, fig.width=8, fig.cap="Variables Importance for fit.gbm model"}

mplot_importance <- function(var, imp, colours = NA, limit = 15, model_name = NA, subtitle = NA,
							 save = FALSE, file_name = "viz_importance.png", subdir = NA) {
	
	require(ggplot2)
	require(gridExtra)
	options(warn=-1)
	
	if (length(var) != length(imp)) {
		message("The variables and importance values vectors should be the same length.")
		stop(message(paste("Currently, there are",length(var),"variables and",length(imp),"importance values!")))
	}
	if (is.na(colours)) {
		colours <- "navy" 
	}
	out <- data.frame(var = var, imp = imp, Type = colours)
	if (length(var) < limit) {
		limit <- length(var)
	}
	
	output <- out[1:limit,]
	
	p <- ggplot(output, 
				aes(x = reorder(var, imp), y = imp, 
					label = round(imp, 2))) + 
		geom_col(aes(fill = Type), width = 0.8) +
		coord_flip() + xlab('') + theme_minimal() +
		ylab('Importance') + 
		geom_text(hjust = 0, size = 3, inherit.aes = TRUE, colour = "black") +
		labs(title = paste0("Variables Importance. (", limit, " / ", length(var), " plotted)"))
	
	if (length(unique(output$Type)) == 1) {
		p <- p + geom_col(fill = colours, width = 0.8) +
			guides(fill = FALSE, colour = FALSE) + 
			geom_text(hjust = 0, size = 3, inherit.aes = TRUE, colour = "black")
	}
	if(!is.na(model_name)) {
		p <- p + labs(caption = model_name)
	}
	if(!is.na(subtitle)) {
		p <- p + labs(subtitle = subtitle)
	}  
	if(save == TRUE) {
		if (!is.na(subdir)) {
			dir.create(file.path(getwd(), subdir))
			file_name <- paste(subdir, file_name, sep="/")
		}
		p <- p + ggsave(file_name, width=7, height=6)
	}
	
	return(p)
	
}

vi<-varImp(fit.gbm)
mplot_importance(var=row.names(vi$importance[1]), imp=vi$importance[[1]], model_name = "fit.gbm", colours = "gold3", limit = 52)

```

An equivalent model could be trained with `r sum(vi$importance[[1]] > 1)` predictors instead of 52, which would reduce the model complexity and allow better scalability. 